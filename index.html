<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Imprinto: Enhancing Infrared Inkjet Watermarking for Human and Machine Perception">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Imprinto: Enhancing Infrared Inkjet Watermarking for Human and Machine Perception</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Imprinto: Enhancing Infrared Inkjet Watermarking for Human and Machine Perception</h1>
          <div class="is-size-5 publication-authors">  
	    <span class="author-block">  
	        <a target="_blank">Martin Feick</a><sup>1,2</sup>,  
	    </span>  
	    <span class="author-block">  
	        <a target="_blank">Xuxin Tang</a><sup>1,3</sup>,  
	    </span>  
	    <span class="author-block">  
	        <a target="_blank">Raul Garcia-Martin</a><sup>1,4</sup>,  
	    </span>  
	    <span class="author-block">  
	        <a target="_blank">Alexandru Luchianov</a><sup>1</sup>,  
	    </span>  
	    <span class="author-block">  
	        <a target="_blank">Roderick Wei Xiao Huang</a><sup>1</sup>,  
	    </span>  
	    <span class="author-block">  
	        <a target="_blank">Chang Xiao</a><sup>5</sup>,  
	    </span>  
	    <span class="author-block">  
	        <a target="_blank">Alexa Siu</a><sup>5</sup>,  
	    </span>  
	    <span class="author-block">  
	        <a target="_blank">Mustafa Doga Dogan</a><sup>5,1</sup>
	    </span>  
	</div>  
	
	<div class="is-size-5 publication-authors">  
	    <span class="author-block"><sup>1</sup> MIT CSAIL, Cambridge, Massachusetts, United States</span><br/>  
	    <span class="author-block"><sup>2</sup> DFKI and Saarland University, Saarbrücken, Germany</span><br/>  
	    <span class="author-block"><sup>3</sup> Virginia Tech, Blacksburg, Virginia, United States</span><br/>  
	    <span class="author-block"><sup>4</sup> Universidad Carlos III de Madrid, Madrid, Spain</span><br/>  
	    <span class="author-block"><sup>5</sup> Adobe Research, Basel, Switzerland</span>  
	    <br/><br/>  
	    <span class="author-block"><a href="https://chi2025.acm.org/" target="_blank">2025 CHI Conference on Human Factors in Computing Systems</a></span>  
	</div>  

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (coming soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub (coming soon)</span>
                  </a>
              </span>
	
	      <span class="link-block">
                <a href="https://www.kaggle.com/code/dogadgn/imprinto-ir-ink-binary-segmentation-of-qr-codes" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-kaggle"></i>
                  </span>
                  <span>Kaggle tutorial</span>
                  </a>
              </span>
           
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

 <!-- Abstract. -->
 <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
      <p> Abstract—Hybrid paper interfaces leverage augmented reality to combine the desired tangibility of paper documents with the affordances of interactive digital media. 
	      Typically, virtual content can be embedded through direct links (e.g., QR codes); however, this impacts the aesthetics of the paper print and limits the available visual content space. 
	      To address this problem, we present Imprinto, an infrared inkjet watermarking technique that allows for invisible content embeddings only by using off-the-shelf IR inks and a camera. 
	      Imprinto was established through a psychophysical experiment, studying how much IR ink can be used while remaining invisible to users regardless of background color. 
	      We demonstrate that we can detect invisible IR content through our machine-learning pipeline, and we developed an authoring tool that optimizes the amount of IR ink on the 
	      color regions of an input document for machine and human detectability. Finally, we demonstrate several applications, including augmenting paper documents and objects. </p>
     
    </div>
  </div>
</div>
<!--/ Abstract. -->



<section class="section">
  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Video</h2>
      <div class="video-container">
        <iframe width="560" height="315" 
                src="https://www.youtube.com/embed/SrE5Jzmhom4" 
                title="Imprinto Paper Video" 
                frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                allowfullscreen>
        </iframe>
      </div>
    </div>
  </div>
  <!--/ Paper video. -->
</section>




<!--
	<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <h2 class="title is-3">Key Features of <span class="dnerf">Rampa</span> </h2>
            <img src="./static/images/main_figure.png"
        class="interpolation-image" width="100%"/>
          </div>
         <h3 class="title is-4">Data Recording</h3>
            <p> Users use AR to demonstrate skills, capturing the data directly in the user's environment, using hand pinch, as shown in the figure above.
              Compared to traditional methods, such as the kinesthetic control, <span class="dnerf">RAMPA</span> simplifies the collection of 
              demonstration trajectories, an integral component of PbD procedures, offering a more efficient experience for 
              robotic programmers.
            </p>
        <h3 class="title is-4">Data Playback and Adjustment</h3>
          <p> <img  style="float: left;padding-right: 30px;" src="./static/images/traj_playback.png" width="450px"/>
            <br/><br/>
            The system allows users to visualize and modify the movements of a virtual robot before these movements are executed 
            on the actual robotic arm. This step ensures that potential errors can be corrected in a safe, virtual space. 
            Moreover, the ability to adjust any portion of demonstration trajectories increases the operator's overall performance.
          </p><br/><br/><br/><br/>

        <h3 class="title is-4">ML Training and Preview</h3>
          <p> <img  style="float: left;padding-right: 30px;" src="./static/images/ml_overview.png" width="450px"/>
            <br/><br/><br/><br/>
            Utilizing the collected data, popular PbD models, such as Probabilistic Movement Primitives (ProMPs), can be trained and validated
            within the AR environment. Users can observe the generated trajectories in real-time, ensuring the robot's behavior aligns with desired outcomes.
          </p><br/>
         </div>
        </div>
      </div>  
    </div>
  </div>
</div>
</section>
-->

<!--
<section class="section">
  
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Implementation</h2>

        <div class="content has-text-justified">
          
          <p>
            XR-Objects  leverages developments in spatial understanding via tools such as  SLAM, 
            available in <a
            href="https://developers.google.com/ar">Google ARCore </a> and 
            <a href="https://developer.apple.com/augmented-reality/arkit">Apple ARKit</a>, 
            and machine learning models for object segmentation and classification (<a href="https://cocodataset.org/COCO">COCO</a> 
            via <a href="https://developers.google.com/mediapipe">MediaPipe</a>), that enable us to implement AR interactions 
            with semantic depth.
            We also integrate a Multimodal Large Language Model (MLLM), 
            <a href="https://deepmind.google/technologies/gemini/">Google Gemini</a>, into our system, 
            which further enhances our ability to automate the recognition of objects and their specific 
            semantic information within XR spaces.  
          </p>
        
          <img poster="" id="fullbody" src="./static/images/pipeline_new.png">
        </div>
      </div>
    </div>
  </div>
</section>
	
-->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{feickImprinto2025,  
  title={Imprinto: Enhancing Infrared Inkjet Watermarking for Human and Machine Perception},  
  author={Feick, Martin and Tang, Xuxin and Garcia-Martin, Raul and Luchianov, Alexandru and Huang, Roderick Wei Xiao and Xiao, Chang and Siu, Alexa and Dogan, Mustafa Doga},  
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI '25)},  
  year={2025},  
  publisher={ACM},  
  address={New York, NY, USA}
}  
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is built on top of the original<a
            href="https://github.com/nerfies/nerfies.github.io">  Nerfies, <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 </a> International License.
          </p>
          </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
